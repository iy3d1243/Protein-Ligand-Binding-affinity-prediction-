{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13146633,"sourceType":"datasetVersion","datasetId":8329295}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport os\nfrom collections import defaultdict\n\ndef remove_high_null_columns_streaming(input_file, output_file, threshold=0.5, delimiter='\\t'):\n    \"\"\"\n    Remove columns with >threshold% null values using minimal memory.\n    Reads file line-by-line without loading into memory.\n    \n    Parameters:\n    - input_file: path to input file\n    - output_file: path to output file  \n    - threshold: fraction of nulls above which to remove column (default 0.5)\n    - delimiter: '\\t' for TSV, ',' for CSV\n    \"\"\"\n    \n    print(\"üîç Starting memory-efficient analysis...\")\n    print(f\"üìÅ Input: {input_file}\")\n    print(f\"üíæ Output: {output_file}\")\n    \n    # Get file size for progress tracking\n    file_size = os.path.getsize(input_file)\n    print(f\"üìä File size: {file_size / (1024**3):.2f} GB\")\n    \n    # First pass: Count nulls and total rows\n    null_counts = defaultdict(int)\n    total_rows = 0\n    column_names = []\n    \n    print(\"\\nüîÑ Pass 1: Counting null values...\")\n    \n    try:\n        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n            reader = csv.reader(f, delimiter=delimiter)\n            \n            # Read header\n            try:\n                header = next(reader)\n                column_names = header\n                num_cols = len(column_names)\n                print(f\"üìã Found {num_cols} columns\")\n                \n                # Initialize null counters\n                for col in column_names:\n                    null_counts[col] = 0\n                    \n            except StopIteration:\n                print(\"‚ùå File is empty or invalid\")\n                return False\n            \n            # Process each row\n            bytes_read = len(','.join(header).encode('utf-8'))\n            \n            for row_num, row in enumerate(reader, 1):\n                total_rows += 1\n                \n                # Pad row if it's shorter than header\n                while len(row) < num_cols:\n                    row.append('')\n                \n                # Count nulls (empty strings, 'NULL', 'NaN', etc.)\n                for i, value in enumerate(row[:num_cols]):\n                    if not value or value.strip().lower() in ['', 'null', 'nan', 'none', 'na']:\n                        if i < len(column_names):\n                            null_counts[column_names[i]] += 1\n                \n                # Progress update every 50k rows\n                if row_num % 50000 == 0:\n                    bytes_read += sum(len(str(cell).encode('utf-8', errors='ignore')) for cell in row)\n                    progress = (bytes_read / file_size) * 100\n                    print(f\"   üìà Processed {row_num:,} rows (~{progress:.1f}% of file)\")\n            \n            print(f\"‚úÖ Pass 1 complete: {total_rows:,} rows analyzed\")\n            \n    except Exception as e:\n        print(f\"‚ùå Error in pass 1: {e}\")\n        return False\n    \n    # Calculate which columns to keep\n    print(f\"\\nüßÆ Calculating null percentages...\")\n    \n    columns_to_keep = []\n    columns_to_remove = []\n    \n    for col in column_names:\n        null_pct = null_counts[col] / total_rows if total_rows > 0 else 0\n        \n        if null_pct <= threshold:\n            columns_to_keep.append(col)\n        else:\n            columns_to_remove.append((col, null_pct))\n    \n    # Sort removed columns by null percentage (worst first)\n    columns_to_remove.sort(key=lambda x: x[1], reverse=True)\n    \n    print(f\"üìä Results:\")\n    print(f\"   Total rows: {total_rows:,}\")\n    print(f\"   Original columns: {len(column_names)}\")\n    print(f\"   Columns to keep: {len(columns_to_keep)}\")\n    print(f\"   Columns to remove: {len(columns_to_remove)}\")\n    \n    if columns_to_remove:\n        print(f\"\\nüóëÔ∏è  Worst offenders (showing top 10):\")\n        for col, pct in columns_to_remove[:10]:\n            print(f\"   - {col}: {pct*100:.1f}% null\")\n        if len(columns_to_remove) > 10:\n            print(f\"   ... and {len(columns_to_remove)-10} more\")\n    \n    # Get indices of columns to keep\n    keep_indices = [i for i, col in enumerate(column_names) if col in columns_to_keep]\n    \n    # Second pass: Write cleaned file\n    print(f\"\\nüíæ Pass 2: Writing cleaned data...\")\n    \n    try:\n        with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \\\n             open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n            \n            reader = csv.reader(infile, delimiter=delimiter)\n            writer = csv.writer(outfile, delimiter=',')  # Output as CSV\n            \n            # Write filtered header\n            header = next(reader)\n            filtered_header = [header[i] for i in keep_indices if i < len(header)]\n            writer.writerow(filtered_header)\n            \n            # Write filtered rows\n            rows_written = 0\n            \n            for row in reader:\n                rows_written += 1\n                \n                # Pad row if necessary\n                while len(row) < len(column_names):\n                    row.append('')\n                \n                # Write only kept columns\n                filtered_row = [row[i] if i < len(row) else '' for i in keep_indices]\n                writer.writerow(filtered_row)\n                \n                # Progress update\n                if rows_written % 100000 == 0:\n                    print(f\"   üìù Written {rows_written:,} rows\")\n            \n            print(f\"‚úÖ Pass 2 complete: {rows_written:,} rows written\")\n            \n    except Exception as e:\n        print(f\"‚ùå Error in pass 2: {e}\")\n        return False\n    \n    # Final summary\n    output_size = os.path.getsize(output_file)\n    reduction = ((file_size - output_size) / file_size) * 100\n    \n    print(f\"\\nüéâ SUCCESS!\")\n    print(f\"   üìÅ Input: {file_size / (1024**2):.1f} MB\")\n    print(f\"   üìÅ Output: {output_size / (1024**2):.1f} MB\") \n    print(f\"   üìâ Size reduction: {reduction:.1f}%\")\n    print(f\"   üóÇÔ∏è  Columns: {len(column_names)} ‚Üí {len(columns_to_keep)}\")\n    print(f\"   üíæ Saved to: {output_file}\")\n    \n    return True\n\ndef quick_file_check(filename):\n    \"\"\"Quick check of file format and size\"\"\"\n    try:\n        size_gb = os.path.getsize(filename) / (1024**3)\n        \n        # Check delimiter by reading first line\n        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n            first_line = f.readline()\n            \n        tabs = first_line.count('\\t')\n        commas = first_line.count(',')\n        \n        delimiter = '\\t' if tabs > commas else ','\n        format_type = 'TSV' if delimiter == '\\t' else 'CSV'\n        \n        print(f\"üìã File Info:\")\n        print(f\"   Size: {size_gb:.2f} GB\")\n        print(f\"   Format: {format_type}\")\n        print(f\"   Delimiter: '{delimiter}'\")\n        \n        return delimiter\n        \n    except Exception as e:\n        print(f\"‚ùå Error checking file: {e}\")\n        return None\n\n# Usage\nif __name__ == \"__main__\":\n    # File paths - UPDATE THESE\n    input_file = \"/kaggle/input/bindingdb-all-202509-tsv/BindingDB_All.tsv\"\n    output_file = \"BindingDB_cleaned.csv\"\n    \n    print(\"üöÄ Ultra Memory-Efficient Column Cleaner\")\n    print(\"=\" * 50)\n    \n    # Check file format\n    delimiter = quick_file_check(input_file)\n    \n    if delimiter:\n        # Process the file\n        success = remove_high_null_columns_streaming(\n            input_file=input_file,\n            output_file=output_file,\n            threshold=0.95,  # Remove columns with >50% nulls\n            delimiter=delimiter\n        )\n        \n        if success:\n            print(\"\\nüéä DONE! Your dataset has been cleaned successfully!\")\n            print(f\"üìÇ Check your cleaned file: {output_file}\")\n        else:\n            print(\"\\nüí• Something went wrong. Check the error messages above.\")\n    else:\n        print(\"‚ùå Could not process file. Check the file path and permissions.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:38:12.948468Z","iopub.execute_input":"2025-09-23T14:38:12.948928Z","iopub.status.idle":"2025-09-23T14:55:28.870602Z","shell.execute_reply.started":"2025-09-23T14:38:12.948890Z","shell.execute_reply":"2025-09-23T14:55:28.869038Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"üöÄ Ultra Memory-Efficient Column Cleaner\n==================================================\nüìã File Info:\n   Size: 6.19 GB\n   Format: TSV\n   Delimiter: '\t'\nüîç Starting memory-efficient analysis...\nüìÅ Input: /kaggle/input/bindingdb-all-202509-tsv/BindingDB_All.tsv\nüíæ Output: BindingDB_cleaned.csv\nüìä File size: 6.19 GB\n\nüîÑ Pass 1: Counting null values...\nüìã Found 640 columns\n   üìà Processed 50,000 rows (~0.0% of file)\n   üìà Processed 100,000 rows (~0.0% of file)\n   üìà Processed 150,000 rows (~0.0% of file)\n   üìà Processed 200,000 rows (~0.0% of file)\n   üìà Processed 250,000 rows (~0.0% of file)\n   üìà Processed 300,000 rows (~0.0% of file)\n   üìà Processed 350,000 rows (~0.0% of file)\n   üìà Processed 400,000 rows (~0.0% of file)\n   üìà Processed 450,000 rows (~0.0% of file)\n   üìà Processed 500,000 rows (~0.0% of file)\n   üìà Processed 550,000 rows (~0.0% of file)\n   üìà Processed 600,000 rows (~0.0% of file)\n   üìà Processed 650,000 rows (~0.0% of file)\n   üìà Processed 700,000 rows (~0.0% of file)\n   üìà Processed 750,000 rows (~0.0% of file)\n   üìà Processed 800,000 rows (~0.0% of file)\n   üìà Processed 850,000 rows (~0.0% of file)\n   üìà Processed 900,000 rows (~0.0% of file)\n   üìà Processed 950,000 rows (~0.0% of file)\n   üìà Processed 1,000,000 rows (~0.0% of file)\n   üìà Processed 1,050,000 rows (~0.0% of file)\n   üìà Processed 1,100,000 rows (~0.0% of file)\n   üìà Processed 1,150,000 rows (~0.0% of file)\n   üìà Processed 1,200,000 rows (~0.0% of file)\n   üìà Processed 1,250,000 rows (~0.0% of file)\n   üìà Processed 1,300,000 rows (~0.0% of file)\n   üìà Processed 1,350,000 rows (~0.0% of file)\n   üìà Processed 1,400,000 rows (~0.0% of file)\n   üìà Processed 1,450,000 rows (~0.0% of file)\n   üìà Processed 1,500,000 rows (~0.0% of file)\n   üìà Processed 1,550,000 rows (~0.0% of file)\n   üìà Processed 1,600,000 rows (~0.0% of file)\n   üìà Processed 1,650,000 rows (~0.0% of file)\n   üìà Processed 1,700,000 rows (~0.0% of file)\n   üìà Processed 1,750,000 rows (~0.0% of file)\n   üìà Processed 1,800,000 rows (~0.0% of file)\n   üìà Processed 1,850,000 rows (~0.0% of file)\n   üìà Processed 1,900,000 rows (~0.0% of file)\n   üìà Processed 1,950,000 rows (~0.0% of file)\n   üìà Processed 2,000,000 rows (~0.0% of file)\n   üìà Processed 2,050,000 rows (~0.0% of file)\n   üìà Processed 2,100,000 rows (~0.0% of file)\n   üìà Processed 2,150,000 rows (~0.0% of file)\n   üìà Processed 2,200,000 rows (~0.0% of file)\n   üìà Processed 2,250,000 rows (~0.0% of file)\n   üìà Processed 2,300,000 rows (~0.0% of file)\n   üìà Processed 2,350,000 rows (~0.0% of file)\n   üìà Processed 2,400,000 rows (~0.0% of file)\n   üìà Processed 2,450,000 rows (~0.0% of file)\n   üìà Processed 2,500,000 rows (~0.0% of file)\n   üìà Processed 2,550,000 rows (~0.0% of file)\n   üìà Processed 2,600,000 rows (~0.0% of file)\n   üìà Processed 2,650,000 rows (~0.0% of file)\n   üìà Processed 2,700,000 rows (~0.0% of file)\n   üìà Processed 2,750,000 rows (~0.0% of file)\n   üìà Processed 2,800,000 rows (~0.0% of file)\n   üìà Processed 2,850,000 rows (~0.0% of file)\n   üìà Processed 2,900,000 rows (~0.0% of file)\n   üìà Processed 2,950,000 rows (~0.0% of file)\n   üìà Processed 3,000,000 rows (~0.0% of file)\n   üìà Processed 3,050,000 rows (~0.0% of file)\n‚úÖ Pass 1 complete: 3,059,280 rows analyzed\n\nüßÆ Calculating null percentages...\nüìä Results:\n   Total rows: 3,059,280\n   Original columns: 640\n   Columns to keep: 42\n   Columns to remove: 598\n\nüóëÔ∏è  Worst offenders (showing top 10):\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 1: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 1: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 2: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 2: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 3: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 3: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 4: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 4: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 5: 100.0% null\n   - UniProt (TrEMBL) Secondary ID(s) of Target Chain 5: 100.0% null\n   ... and 588 more\n\nüíæ Pass 2: Writing cleaned data...\n   üìù Written 100,000 rows\n   üìù Written 200,000 rows\n   üìù Written 300,000 rows\n   üìù Written 400,000 rows\n   üìù Written 500,000 rows\n   üìù Written 600,000 rows\n   üìù Written 700,000 rows\n   üìù Written 800,000 rows\n   üìù Written 900,000 rows\n   üìù Written 1,000,000 rows\n   üìù Written 1,100,000 rows\n   üìù Written 1,200,000 rows\n   üìù Written 1,300,000 rows\n   üìù Written 1,400,000 rows\n   üìù Written 1,500,000 rows\n   üìù Written 1,600,000 rows\n   üìù Written 1,700,000 rows\n   üìù Written 1,800,000 rows\n   üìù Written 1,900,000 rows\n   üìù Written 2,000,000 rows\n   üìù Written 2,100,000 rows\n   üìù Written 2,200,000 rows\n   üìù Written 2,300,000 rows\n   üìù Written 2,400,000 rows\n   üìù Written 2,500,000 rows\n   üìù Written 2,600,000 rows\n   üìù Written 2,700,000 rows\n   üìù Written 2,800,000 rows\n   üìù Written 2,900,000 rows\n   üìù Written 3,000,000 rows\n‚úÖ Pass 2 complete: 3,059,280 rows written\n\nüéâ SUCCESS!\n   üìÅ Input: 6338.2 MB\n   üìÅ Output: 6223.5 MB\n   üìâ Size reduction: 1.8%\n   üóÇÔ∏è  Columns: 640 ‚Üí 42\n   üíæ Saved to: BindingDB_cleaned.csv\n\nüéä DONE! Your dataset has been cleaned successfully!\nüìÇ Check your cleaned file: BindingDB_cleaned.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# show all rows (no truncation)\npd.set_option(\"display.max_rows\", None)\n\n# optional: show all columns if needed\npd.set_option(\"display.max_columns\", None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:04:34.053477Z","iopub.execute_input":"2025-09-23T15:04:34.053867Z","iopub.status.idle":"2025-09-23T15:04:34.059302Z","shell.execute_reply.started":"2025-09-23T15:04:34.053834Z","shell.execute_reply":"2025-09-23T15:04:34.058103Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd \ndf=pd.read_csv('/kaggle/working/BindingDB_cleaned.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:02:41.115501Z","iopub.execute_input":"2025-09-23T15:02:41.117126Z","iopub.status.idle":"2025-09-23T15:04:28.279663Z","shell.execute_reply.started":"2025-09-23T15:02:41.117065Z","shell.execute_reply":"2025-09-23T15:04:28.278658Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_77/1058850448.py:2: DtypeWarning: Columns (8,10,12,14,17,18,30,38,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n  df=pd.read_csv('/kaggle/working/BindingDB_cleaned.csv')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:04:46.133565Z","iopub.execute_input":"2025-09-23T15:04:46.134823Z","iopub.status.idle":"2025-09-23T15:04:50.944697Z","shell.execute_reply.started":"2025-09-23T15:04:46.134779Z","shell.execute_reply":"2025-09-23T15:04:50.943755Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BindingDB Reactant_set_id                                                     0\nLigand SMILES                                                                11\nLigand InChI                                                             174513\nLigand InChI Key                                                         174513\nBindingDB MonomerID                                                           0\nBindingDB Ligand Name                                                         0\nTarget Name                                                                   0\nTarget Source Organism According to Curator or DataSource                947169\nKi (nM)                                                                 2449633\nIC50 (nM)                                                               1002257\nEC50 (nM)                                                               2782660\npH                                                                      2844019\nTemp (C)                                                                2858376\nCuration/DataSource                                                           0\nArticle DOI                                                             1461073\nBindingDB Entry DOI                                                        6986\nPMID                                                                    1397369\nPubChem AID                                                             1902009\nPatent Number                                                           1849819\nAuthors                                                                   87851\nDate of publication                                                           0\nDate in BindingDB                                                             0\nInstitution                                                               87851\nLink to Ligand in BindingDB                                                   0\nLink to Target in BindingDB                                                   0\nLink to Ligand-Target Pair in BindingDB                                       0\nLigand HET ID in PDB                                                    2889375\nPubChem CID                                                               78143\nPubChem SID                                                               78011\nChEMBL ID of Ligand                                                     1948490\nZINC ID of Ligand                                                       2079034\nNumber of Protein Chains in Target (>1 implies a multichain complex)          0\nBindingDB Target Chain Sequence 1                                            99\nPDB ID(s) of Target Chain 1                                              634670\nUniProt (SwissProt) Recommended Name of Target Chain 1                    89759\nUniProt (SwissProt) Entry Name of Target Chain 1                          90567\nUniProt (SwissProt) Primary ID of Target Chain 1                          89759\nUniProt (SwissProt) Secondary ID(s) of Target Chain 1                    388222\nBindingDB Target Chain Sequence 2                                       2890125\nUniProt (SwissProt) Recommended Name of Target Chain 2                  2892001\nUniProt (SwissProt) Entry Name of Target Chain 2                        2892001\nUniProt (SwissProt) Primary ID of Target Chain 2                        2892001\ndtype: int64"},"metadata":{}}],"execution_count":9}]}