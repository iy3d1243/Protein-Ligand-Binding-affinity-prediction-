{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13146633,"sourceType":"datasetVersion","datasetId":8329295}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport os\nfrom collections import defaultdict\n\ndef remove_high_null_columns_streaming(input_file, output_file, threshold=0.5, delimiter='\\t'):\n    \"\"\"\n    Remove columns with >threshold% null values using minimal memory.\n    Reads file line-by-line without loading into memory.\n    \n    Parameters:\n    - input_file: path to input file\n    - output_file: path to output file  \n    - threshold: fraction of nulls above which to remove column (default 0.5)\n    - delimiter: '\\t' for TSV, ',' for CSV\n    \"\"\"\n    \n    print(\"🔍 Starting memory-efficient analysis...\")\n    print(f\"📁 Input: {input_file}\")\n    print(f\"💾 Output: {output_file}\")\n    \n    # Get file size for progress tracking\n    file_size = os.path.getsize(input_file)\n    print(f\"📊 File size: {file_size / (1024**3):.2f} GB\")\n    \n    # First pass: Count nulls and total rows\n    null_counts = defaultdict(int)\n    total_rows = 0\n    column_names = []\n    \n    print(\"\\n🔄 Pass 1: Counting null values...\")\n    \n    try:\n        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n            reader = csv.reader(f, delimiter=delimiter)\n            \n            # Read header\n            try:\n                header = next(reader)\n                column_names = header\n                num_cols = len(column_names)\n                print(f\"📋 Found {num_cols} columns\")\n                \n                # Initialize null counters\n                for col in column_names:\n                    null_counts[col] = 0\n                    \n            except StopIteration:\n                print(\"❌ File is empty or invalid\")\n                return False\n            \n            # Process each row\n            bytes_read = len(','.join(header).encode('utf-8'))\n            \n            for row_num, row in enumerate(reader, 1):\n                total_rows += 1\n                \n                # Pad row if it's shorter than header\n                while len(row) < num_cols:\n                    row.append('')\n                \n                # Count nulls (empty strings, 'NULL', 'NaN', etc.)\n                for i, value in enumerate(row[:num_cols]):\n                    if not value or value.strip().lower() in ['', 'null', 'nan', 'none', 'na']:\n                        if i < len(column_names):\n                            null_counts[column_names[i]] += 1\n                \n                # Progress update every 50k rows\n                if row_num % 50000 == 0:\n                    bytes_read += sum(len(str(cell).encode('utf-8', errors='ignore')) for cell in row)\n                    progress = (bytes_read / file_size) * 100\n                    print(f\"   📈 Processed {row_num:,} rows (~{progress:.1f}% of file)\")\n            \n            print(f\"✅ Pass 1 complete: {total_rows:,} rows analyzed\")\n            \n    except Exception as e:\n        print(f\"❌ Error in pass 1: {e}\")\n        return False\n    \n    # Calculate which columns to keep\n    print(f\"\\n🧮 Calculating null percentages...\")\n    \n    columns_to_keep = []\n    columns_to_remove = []\n    \n    for col in column_names:\n        null_pct = null_counts[col] / total_rows if total_rows > 0 else 0\n        \n        if null_pct <= threshold:\n            columns_to_keep.append(col)\n        else:\n            columns_to_remove.append((col, null_pct))\n    \n    # Sort removed columns by null percentage (worst first)\n    columns_to_remove.sort(key=lambda x: x[1], reverse=True)\n    \n    print(f\"📊 Results:\")\n    print(f\"   Total rows: {total_rows:,}\")\n    print(f\"   Original columns: {len(column_names)}\")\n    print(f\"   Columns to keep: {len(columns_to_keep)}\")\n    print(f\"   Columns to remove: {len(columns_to_remove)}\")\n    \n    if columns_to_remove:\n        print(f\"\\n🗑️  Worst offenders (showing top 10):\")\n        for col, pct in columns_to_remove[:10]:\n            print(f\"   - {col}: {pct*100:.1f}% null\")\n        if len(columns_to_remove) > 10:\n            print(f\"   ... and {len(columns_to_remove)-10} more\")\n    \n    # Get indices of columns to keep\n    keep_indices = [i for i, col in enumerate(column_names) if col in columns_to_keep]\n    \n    # Second pass: Write cleaned file\n    print(f\"\\n💾 Pass 2: Writing cleaned data...\")\n    \n    try:\n        with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \\\n             open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n            \n            reader = csv.reader(infile, delimiter=delimiter)\n            writer = csv.writer(outfile, delimiter=',')  # Output as CSV\n            \n            # Write filtered header\n            header = next(reader)\n            filtered_header = [header[i] for i in keep_indices if i < len(header)]\n            writer.writerow(filtered_header)\n            \n            # Write filtered rows\n            rows_written = 0\n            \n            for row in reader:\n                rows_written += 1\n                \n                # Pad row if necessary\n                while len(row) < len(column_names):\n                    row.append('')\n                \n                # Write only kept columns\n                filtered_row = [row[i] if i < len(row) else '' for i in keep_indices]\n                writer.writerow(filtered_row)\n                \n                # Progress update\n                if rows_written % 100000 == 0:\n                    print(f\"   📝 Written {rows_written:,} rows\")\n            \n            print(f\"✅ Pass 2 complete: {rows_written:,} rows written\")\n            \n    except Exception as e:\n        print(f\"❌ Error in pass 2: {e}\")\n        return False\n    \n    # Final summary\n    output_size = os.path.getsize(output_file)\n    reduction = ((file_size - output_size) / file_size) * 100\n    \n    print(f\"\\n🎉 SUCCESS!\")\n    print(f\"   📁 Input: {file_size / (1024**2):.1f} MB\")\n    print(f\"   📁 Output: {output_size / (1024**2):.1f} MB\") \n    print(f\"   📉 Size reduction: {reduction:.1f}%\")\n    print(f\"   🗂️  Columns: {len(column_names)} → {len(columns_to_keep)}\")\n    print(f\"   💾 Saved to: {output_file}\")\n    \n    return True\n\ndef quick_file_check(filename):\n    \"\"\"Quick check of file format and size\"\"\"\n    try:\n        size_gb = os.path.getsize(filename) / (1024**3)\n        \n        # Check delimiter by reading first line\n        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n            first_line = f.readline()\n            \n        tabs = first_line.count('\\t')\n        commas = first_line.count(',')\n        \n        delimiter = '\\t' if tabs > commas else ','\n        format_type = 'TSV' if delimiter == '\\t' else 'CSV'\n        \n        print(f\"📋 File Info:\")\n        print(f\"   Size: {size_gb:.2f} GB\")\n        print(f\"   Format: {format_type}\")\n        print(f\"   Delimiter: '{delimiter}'\")\n        \n        return delimiter\n        \n    except Exception as e:\n        print(f\"❌ Error checking file: {e}\")\n        return None\n\n# Usage\nif __name__ == \"__main__\":\n    # File paths - UPDATE THESE\n    input_file = \"/kaggle/input/bindingdb-all-202509-tsv/BindingDB_All.tsv\"\n    output_file = \"BindingDB_cleaned.csv\"\n    \n    print(\"🚀 Ultra Memory-Efficient Column Cleaner\")\n    print(\"=\" * 50)\n    \n    # Check file format\n    delimiter = quick_file_check(input_file)\n    \n    if delimiter:\n        # Process the file\n        success = remove_high_null_columns_streaming(\n            input_file=input_file,\n            output_file=output_file,\n            threshold=0.95,  # Remove columns with >50% nulls\n            delimiter=delimiter\n        )\n        \n        if success:\n            print(\"\\n🎊 DONE! Your dataset has been cleaned successfully!\")\n            print(f\"📂 Check your cleaned file: {output_file}\")\n        else:\n            print(\"\\n💥 Something went wrong. Check the error messages above.\")\n    else:\n        print(\"❌ Could not process file. Check the file path and permissions.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:38:12.948468Z","iopub.execute_input":"2025-09-23T14:38:12.948928Z","iopub.status.idle":"2025-09-23T14:55:28.870602Z","shell.execute_reply.started":"2025-09-23T14:38:12.948890Z","shell.execute_reply":"2025-09-23T14:55:28.869038Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"🚀 Ultra Memory-Efficient Column Cleaner\n==================================================\n📋 File Info:\n   Size: 6.19 GB\n   Format: TSV\n   Delimiter: '\t'\n🔍 Starting memory-efficient analysis...\n📁 Input: /kaggle/input/bindingdb-all-202509-tsv/BindingDB_All.tsv\n💾 Output: BindingDB_cleaned.csv\n📊 File size: 6.19 GB\n\n🔄 Pass 1: Counting null values...\n📋 Found 640 columns\n   📈 Processed 50,000 rows (~0.0% of file)\n   📈 Processed 100,000 rows (~0.0% of file)\n   📈 Processed 150,000 rows (~0.0% of file)\n   📈 Processed 200,000 rows (~0.0% of file)\n   📈 Processed 250,000 rows (~0.0% of file)\n   📈 Processed 300,000 rows (~0.0% of file)\n   📈 Processed 350,000 rows (~0.0% of file)\n   📈 Processed 400,000 rows (~0.0% of file)\n   📈 Processed 450,000 rows (~0.0% of file)\n   📈 Processed 500,000 rows (~0.0% of file)\n   📈 Processed 550,000 rows (~0.0% of file)\n   📈 Processed 600,000 rows (~0.0% of file)\n   📈 Processed 650,000 rows (~0.0% of file)\n   📈 Processed 700,000 rows (~0.0% of file)\n   📈 Processed 750,000 rows (~0.0% of file)\n   📈 Processed 800,000 rows (~0.0% of file)\n   📈 Processed 850,000 rows (~0.0% of file)\n   📈 Processed 900,000 rows (~0.0% of file)\n   📈 Processed 950,000 rows (~0.0% of file)\n   📈 Processed 1,000,000 rows (~0.0% of file)\n   📈 Processed 1,050,000 rows (~0.0% of file)\n   📈 Processed 1,100,000 rows (~0.0% of file)\n   📈 Processed 1,150,000 rows (~0.0% of file)\n   📈 Processed 1,200,000 rows (~0.0% of file)\n   📈 Processed 1,250,000 rows (~0.0% of file)\n   📈 Processed 1,300,000 rows (~0.0% of file)\n   📈 Processed 1,350,000 rows (~0.0% of file)\n   📈 Processed 1,400,000 rows (~0.0% of file)\n   📈 Processed 1,450,000 rows (~0.0% of file)\n   📈 Processed 1,500,000 rows (~0.0% of file)\n   📈 Processed 1,550,000 rows (~0.0% of file)\n   📈 Processed 1,600,000 rows (~0.0% of file)\n   📈 Processed 1,650,000 rows (~0.0% of file)\n   📈 Processed 1,700,000 rows (~0.0% of file)\n   📈 Processed 1,750,000 rows (~0.0% of file)\n   📈 Processed 1,800,000 rows (~0.0% of file)\n   📈 Processed 1,850,000 rows (~0.0% of file)\n   📈 Processed 1,900,000 rows (~0.0% of file)\n   📈 Processed 1,950,000 rows (~0.0% of file)\n   📈 Processed 2,000,000 rows (~0.0% of file)\n   📈 Processed 2,050,000 rows (~0.0% of file)\n   📈 Processed 2,100,000 rows (~0.0% of file)\n   📈 Processed 2,150,000 rows (~0.0% of file)\n   📈 Processed 2,200,000 rows (~0.0% of file)\n   📈 Processed 2,250,000 rows (~0.0% of file)\n   📈 Processed 2,300,000 rows (~0.0% of file)\n   📈 Processed 2,350,000 rows (~0.0% of file)\n   📈 Processed 2,400,000 rows (~0.0% of file)\n   📈 Processed 2,450,000 rows (~0.0% of file)\n   📈 Processed 2,500,000 rows (~0.0% of file)\n   📈 Processed 2,550,000 rows (~0.0% of file)\n   📈 Processed 2,600,000 rows (~0.0% of file)\n   📈 Processed 2,650,000 rows (~0.0% of file)\n   📈 Processed 2,700,000 rows (~0.0% of file)\n   📈 Processed 2,750,000 rows (~0.0% of file)\n   📈 Processed 2,800,000 rows (~0.0% of file)\n   📈 Processed 2,850,000 rows (~0.0% of file)\n   📈 Processed 2,900,000 rows (~0.0% of file)\n   📈 Processed 2,950,000 rows (~0.0% of file)\n   📈 Processed 3,000,000 rows (~0.0% of file)\n   📈 Processed 3,050,000 rows (~0.0% of file)\n✅ Pass 1 complete: 3,059,280 rows analyzed\n\n🧮 Calculating null percentages...\n📊 Results:\n   Total rows: 3,059,280\n   Original columns: 640\n   Columns to keep: 42\n   Columns to remove: 598\n\n🗑️  Worst offenders (showing top 10):\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 1: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 1: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 2: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 2: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 3: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 3: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 4: 100.0% null\n   - UniProt (TrEMBL) Alternative ID(s) of Target Chain 4: 100.0% null\n   - UniProt (SwissProt) Alternative ID(s) of Target Chain 5: 100.0% null\n   - UniProt (TrEMBL) Secondary ID(s) of Target Chain 5: 100.0% null\n   ... and 588 more\n\n💾 Pass 2: Writing cleaned data...\n   📝 Written 100,000 rows\n   📝 Written 200,000 rows\n   📝 Written 300,000 rows\n   📝 Written 400,000 rows\n   📝 Written 500,000 rows\n   📝 Written 600,000 rows\n   📝 Written 700,000 rows\n   📝 Written 800,000 rows\n   📝 Written 900,000 rows\n   📝 Written 1,000,000 rows\n   📝 Written 1,100,000 rows\n   📝 Written 1,200,000 rows\n   📝 Written 1,300,000 rows\n   📝 Written 1,400,000 rows\n   📝 Written 1,500,000 rows\n   📝 Written 1,600,000 rows\n   📝 Written 1,700,000 rows\n   📝 Written 1,800,000 rows\n   📝 Written 1,900,000 rows\n   📝 Written 2,000,000 rows\n   📝 Written 2,100,000 rows\n   📝 Written 2,200,000 rows\n   📝 Written 2,300,000 rows\n   📝 Written 2,400,000 rows\n   📝 Written 2,500,000 rows\n   📝 Written 2,600,000 rows\n   📝 Written 2,700,000 rows\n   📝 Written 2,800,000 rows\n   📝 Written 2,900,000 rows\n   📝 Written 3,000,000 rows\n✅ Pass 2 complete: 3,059,280 rows written\n\n🎉 SUCCESS!\n   📁 Input: 6338.2 MB\n   📁 Output: 6223.5 MB\n   📉 Size reduction: 1.8%\n   🗂️  Columns: 640 → 42\n   💾 Saved to: BindingDB_cleaned.csv\n\n🎊 DONE! Your dataset has been cleaned successfully!\n📂 Check your cleaned file: BindingDB_cleaned.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# show all rows (no truncation)\npd.set_option(\"display.max_rows\", None)\n\n# optional: show all columns if needed\npd.set_option(\"display.max_columns\", None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:04:34.053477Z","iopub.execute_input":"2025-09-23T15:04:34.053867Z","iopub.status.idle":"2025-09-23T15:04:34.059302Z","shell.execute_reply.started":"2025-09-23T15:04:34.053834Z","shell.execute_reply":"2025-09-23T15:04:34.058103Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd \ndf=pd.read_csv('/kaggle/working/BindingDB_cleaned.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:02:41.115501Z","iopub.execute_input":"2025-09-23T15:02:41.117126Z","iopub.status.idle":"2025-09-23T15:04:28.279663Z","shell.execute_reply.started":"2025-09-23T15:02:41.117065Z","shell.execute_reply":"2025-09-23T15:04:28.278658Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_77/1058850448.py:2: DtypeWarning: Columns (8,10,12,14,17,18,30,38,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n  df=pd.read_csv('/kaggle/working/BindingDB_cleaned.csv')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:04:46.133565Z","iopub.execute_input":"2025-09-23T15:04:46.134823Z","iopub.status.idle":"2025-09-23T15:04:50.944697Z","shell.execute_reply.started":"2025-09-23T15:04:46.134779Z","shell.execute_reply":"2025-09-23T15:04:50.943755Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BindingDB Reactant_set_id                                                     0\nLigand SMILES                                                                11\nLigand InChI                                                             174513\nLigand InChI Key                                                         174513\nBindingDB MonomerID                                                           0\nBindingDB Ligand Name                                                         0\nTarget Name                                                                   0\nTarget Source Organism According to Curator or DataSource                947169\nKi (nM)                                                                 2449633\nIC50 (nM)                                                               1002257\nEC50 (nM)                                                               2782660\npH                                                                      2844019\nTemp (C)                                                                2858376\nCuration/DataSource                                                           0\nArticle DOI                                                             1461073\nBindingDB Entry DOI                                                        6986\nPMID                                                                    1397369\nPubChem AID                                                             1902009\nPatent Number                                                           1849819\nAuthors                                                                   87851\nDate of publication                                                           0\nDate in BindingDB                                                             0\nInstitution                                                               87851\nLink to Ligand in BindingDB                                                   0\nLink to Target in BindingDB                                                   0\nLink to Ligand-Target Pair in BindingDB                                       0\nLigand HET ID in PDB                                                    2889375\nPubChem CID                                                               78143\nPubChem SID                                                               78011\nChEMBL ID of Ligand                                                     1948490\nZINC ID of Ligand                                                       2079034\nNumber of Protein Chains in Target (>1 implies a multichain complex)          0\nBindingDB Target Chain Sequence 1                                            99\nPDB ID(s) of Target Chain 1                                              634670\nUniProt (SwissProt) Recommended Name of Target Chain 1                    89759\nUniProt (SwissProt) Entry Name of Target Chain 1                          90567\nUniProt (SwissProt) Primary ID of Target Chain 1                          89759\nUniProt (SwissProt) Secondary ID(s) of Target Chain 1                    388222\nBindingDB Target Chain Sequence 2                                       2890125\nUniProt (SwissProt) Recommended Name of Target Chain 2                  2892001\nUniProt (SwissProt) Entry Name of Target Chain 2                        2892001\nUniProt (SwissProt) Primary ID of Target Chain 2                        2892001\ndtype: int64"},"metadata":{}}],"execution_count":9}]}