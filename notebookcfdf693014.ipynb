{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13327866,"sourceType":"datasetVersion","datasetId":8449686},{"sourceId":13359654,"sourceType":"datasetVersion","datasetId":8473847}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch_geometric pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__ )').html","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:38:04.706946Z","iopub.execute_input":"2025-10-13T14:38:04.707256Z","iopub.status.idle":"2025-10-13T14:38:15.757404Z","shell.execute_reply.started":"2025-10-13T14:38:04.707233Z","shell.execute_reply":"2025-10-13T14:38:15.756683Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:38:15.758711Z","iopub.execute_input":"2025-10-13T14:38:15.758940Z","iopub.status.idle":"2025-10-13T14:38:21.045367Z","shell.execute_reply.started":"2025-10-13T14:38:15.758921Z","shell.execute_reply":"2025-10-13T14:38:21.044623Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:38:21.046388Z","iopub.execute_input":"2025-10-13T14:38:21.046697Z","iopub.status.idle":"2025-10-13T14:38:25.315702Z","shell.execute_reply.started":"2025-10-13T14:38:21.046669Z","shell.execute_reply":"2025-10-13T14:38:25.314803Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q pandas scikit-learn tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:38:25.317736Z","iopub.execute_input":"2025-10-13T14:38:25.317985Z","iopub.status.idle":"2025-10-13T14:38:28.637608Z","shell.execute_reply.started":"2025-10-13T14:38:25.317964Z","shell.execute_reply":"2025-10-13T14:38:28.636539Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nOPTIMIZED FOR MULTI-GPU TRAINING with DistributedDataParallel\n- Proper DDP setup for PyTorch Geometric\n- Distributed data loading with DistributedSampler\n- Gradient synchronization across GPUs\n- Persistent preprocessed data (saved to disk)\n- Memory-efficient training with mixed precision\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch_geometric.data import Data, Batch\nimport pandas as pd\nimport numpy as np\nfrom rdkit import Chem\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom transformers import EsmModel, AutoTokenizer\nimport warnings\nfrom tqdm import tqdm\nimport os\nimport pickle\nfrom pathlib import Path\n\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIGURATION - OPTIMIZED FOR MULTI-GPU DDP\n# ============================================================================\n\nclass Config:\n    \"\"\"Centralized configuration for distributed training\"\"\"\n    # Model architecture\n    LIGAND_EMBED_DIM = 192\n    PROTEIN_EMBED_DIM = 192\n    GNN_HIDDEN_DIM = 96\n    GNN_NUM_LAYERS = 2\n    ATTENTION_HEADS = 6\n    ATTENTION_DROPOUT = 0.15\n    MLP_DIMS = [576, 384, 192, 64, 1]\n    MLP_DROPOUT = 0.2\n    \n    # ESM-2 settings\n    ESM_MODEL = \"facebook/esm2_t12_35M_UR50D\"\n    ESM_MAX_LENGTH = 800\n    FREEZE_ESM = True\n    \n    # Training - DDP optimized\n    BATCH_SIZE = 8           # Per GPU batch size\n    GRADIENT_ACCUMULATION = 2  # Accumulation steps per GPU\n    LEARNING_RATE = 3e-5\n    WEIGHT_DECAY = 1e-5\n    NUM_EPOCHS = 50\n    PATIENCE = 15\n    GRAD_CLIP_NORM = 1.0\n    \n    # Data\n    TEST_SIZE = 0.15\n    VAL_SIZE = 0.15\n    RANDOM_SEED = 42\n    \n    # Persistence paths\n    DATA_CACHE_DIR = Path(\"/kaggle/working/dta_cache\")\n    PROCESSED_DATA_FILE = \"processed_datasets.pkl\"\n    \n    # DDP settings\n    BACKEND = 'nccl'  # Use 'gloo' for CPU or Windows\n    FIND_UNUSED_PARAMETERS = False  # Set to True if needed\n    \n    # DataLoader settings\n    NUM_WORKERS = 4  # Per GPU\n    PIN_MEMORY = True\n    PERSISTENT_WORKERS = False  # Set to False for DDP compatibility\n\n# ============================================================================\n# DDP UTILITY FUNCTIONS\n# ============================================================================\n\ndef setup_ddp(rank, world_size):\n    \"\"\"Initialize the distributed environment\"\"\"\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    \n    # Initialize process group\n    dist.init_process_group(\n        backend=Config.BACKEND,\n        init_method='env://',\n        world_size=world_size,\n        rank=rank\n    )\n    \n    # Set device for this process\n    torch.cuda.set_device(rank)\n    \n    # Set random seeds for reproducibility\n    torch.manual_seed(Config.RANDOM_SEED + rank)\n    np.random.seed(Config.RANDOM_SEED + rank)\n\n\ndef cleanup_ddp():\n    \"\"\"Clean up distributed environment\"\"\"\n    dist.destroy_process_group()\n\n\ndef is_main_process():\n    \"\"\"Check if this is the main process (rank 0)\"\"\"\n    return not dist.is_initialized() or dist.get_rank() == 0\n\n\ndef get_rank():\n    \"\"\"Get current process rank\"\"\"\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef get_world_size():\n    \"\"\"Get total number of processes\"\"\"\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef reduce_metric(value, world_size):\n    \"\"\"Average a metric across all processes\"\"\"\n    if not dist.is_initialized():\n        return value\n    \n    tensor = torch.tensor(value, device=f'cuda:{get_rank()}')\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    return tensor.item() / world_size\n\n\ndef all_gather_predictions(predictions, labels):\n    \"\"\"Gather predictions and labels from all processes\"\"\"\n    if not dist.is_initialized():\n        return predictions, labels\n    \n    world_size = get_world_size()\n    rank = get_rank()\n    \n    # Convert to tensors\n    pred_tensor = torch.tensor(predictions, device=f'cuda:{rank}')\n    label_tensor = torch.tensor(labels, device=f'cuda:{rank}')\n    \n    # Gather sizes from all processes\n    local_size = torch.tensor([len(predictions)], device=f'cuda:{rank}')\n    size_list = [torch.zeros(1, dtype=torch.long, device=f'cuda:{rank}') \n                 for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    \n    max_size = max([s.item() for s in size_list])\n    \n    # Pad tensors to max size\n    padded_pred = torch.zeros(max_size, device=f'cuda:{rank}')\n    padded_label = torch.zeros(max_size, device=f'cuda:{rank}')\n    padded_pred[:len(predictions)] = pred_tensor\n    padded_label[:len(labels)] = label_tensor\n    \n    # Gather from all processes\n    pred_list = [torch.zeros(max_size, device=f'cuda:{rank}') \n                 for _ in range(world_size)]\n    label_list = [torch.zeros(max_size, device=f'cuda:{rank}') \n                  for _ in range(world_size)]\n    \n    dist.all_gather(pred_list, padded_pred)\n    dist.all_gather(label_list, padded_label)\n    \n    # Concatenate and trim padding\n    all_preds = []\n    all_labels = []\n    for i, size in enumerate(size_list):\n        all_preds.extend(pred_list[i][:size.item()].cpu().numpy())\n        all_labels.extend(label_list[i][:size.item()].cpu().numpy())\n    \n    return np.array(all_preds), np.array(all_labels)\n\n# ============================================================================\n# DATA PROCESSING (Same as original with minor fixes)\n# ============================================================================\n\ndef smiles_to_graph(smiles):\n    \"\"\"Convert SMILES to PyTorch Geometric graph\"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None\n    \n    atom_features = []\n    for atom in mol.GetAtoms():\n        features = [\n            atom.GetAtomicNum(),\n            atom.GetDegree(),\n            atom.GetFormalCharge(),\n            atom.GetHybridization().real,\n            int(atom.GetIsAromatic())\n        ]\n        atom_features.append(features)\n    \n    x = torch.tensor(atom_features, dtype=torch.float)\n    \n    edge_indices = []\n    edge_features = []\n    for bond in mol.GetBonds():\n        i = bond.GetBeginAtomIdx()\n        j = bond.GetEndAtomIdx()\n        bond_type = bond.GetBondTypeAsDouble()\n        edge_indices.extend([[i, j], [j, i]])\n        edge_features.extend([bond_type, bond_type])\n    \n    if len(edge_indices) == 0:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n        edge_attr = torch.zeros((0, 1), dtype=torch.float)\n    else:\n        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n        edge_attr = torch.tensor(edge_features, dtype=torch.float).unsqueeze(1)\n    \n    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n\n\nclass DTADataset(Dataset):\n    \"\"\"Drug-Target Affinity Dataset with disk caching\"\"\"\n    \n    def __init__(self, df, tokenizer, max_length=800, normalize_pki=True, \n                 pki_mean=None, pki_std=None, cache_data=None):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.normalize_pki = normalize_pki\n        \n        if normalize_pki:\n            if pki_mean is None:\n                self.pki_mean = df['pKi'].mean()\n                self.pki_std = df['pKi'].std()\n            else:\n                self.pki_mean = pki_mean\n                self.pki_std = pki_std\n        \n        if cache_data is not None:\n            if is_main_process():\n                print(\"✓ Loading from cache...\")\n            self.graphs = cache_data['graphs']\n            self.protein_tokens = cache_data['protein_tokens']\n            self.df = cache_data['df']\n        else:\n            self._process_data()\n    \n    def _process_data(self):\n        \"\"\"Process SMILES and proteins\"\"\"\n        valid_indices = []\n        self.graphs = []\n        \n        if is_main_process():\n            print(\"Processing SMILES to molecular graphs...\")\n        \n        for idx, smiles in enumerate(tqdm(self.df['Ligand SMILES'], \n                                         desc=\"SMILES\", \n                                         disable=not is_main_process())):\n            graph = smiles_to_graph(smiles)\n            if graph is not None:\n                self.graphs.append(graph)\n                valid_indices.append(idx)\n        \n        self.df = self.df.iloc[valid_indices].reset_index(drop=True)\n        \n        if is_main_process():\n            print(f\"✓ Valid molecules: {len(self.df)}\")\n            print(\"Pre-tokenizing protein sequences...\")\n        \n        self.protein_tokens = []\n        for idx in tqdm(range(len(self.df)), \n                       desc=\"Proteins\", \n                       disable=not is_main_process()):\n            row = self.df.iloc[idx]\n            sequence = self._get_protein_sequence(row)\n            \n            tokens = self.tokenizer(\n                sequence,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            self.protein_tokens.append({\n                'input_ids': tokens['input_ids'].squeeze(0),\n                'attention_mask': tokens['attention_mask'].squeeze(0)\n            })\n        \n        if is_main_process():\n            print(f\"✓ Pre-processing complete!\\n\")\n    \n    def _get_protein_sequence(self, row):\n        \"\"\"Find protein sequence from various possible column names\"\"\"\n        possible_names = [\n            'BindingDB Target Chain Sequence',\n            'BindingDB Target Chain Sequence 1',\n            'Protein Sequence',\n            'protein_sequence',\n            'sequence'\n        ]\n        \n        for name in possible_names:\n            if name in row.index:\n                return row[name]\n        \n        seq_cols = [col for col in row.index if 'sequence' in col.lower()]\n        if seq_cols:\n            return row[seq_cols[0]]\n        \n        raise KeyError(f\"Cannot find protein sequence column. Available: {list(row.index)}\")\n    \n    def get_cache_data(self):\n        \"\"\"Return data for caching\"\"\"\n        return {\n            'graphs': self.graphs,\n            'protein_tokens': self.protein_tokens,\n            'df': self.df\n        }\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        graph = self.graphs[idx]\n        protein_input_ids = self.protein_tokens[idx]['input_ids']\n        protein_attention_mask = self.protein_tokens[idx]['attention_mask']\n        \n        pki = row['pKi']\n        if self.normalize_pki:\n            pki = (pki - self.pki_mean) / self.pki_std\n        pki = torch.tensor(pki, dtype=torch.float)\n        \n        return {\n            'graph': graph,\n            'protein_input_ids': protein_input_ids,\n            'protein_attention_mask': protein_attention_mask,\n            'pki': pki\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate for batching PyTorch Geometric data\"\"\"\n    graphs = [item['graph'] for item in batch]\n    graph_batch = Batch.from_data_list(graphs)\n    \n    protein_input_ids = torch.stack([item['protein_input_ids'] for item in batch])\n    protein_attention_mask = torch.stack([item['protein_attention_mask'] for item in batch])\n    pki = torch.stack([item['pki'] for item in batch])\n    \n    return {\n        'graph_batch': graph_batch,\n        'protein_input_ids': protein_input_ids,\n        'protein_attention_mask': protein_attention_mask,\n        'pki': pki\n    }\n\n# ============================================================================\n# MODEL ARCHITECTURE (Same as original)\n# ============================================================================\n\nclass LigandGNN(nn.Module):\n    \"\"\"Graph Neural Network for ligand encoding\"\"\"\n    \n    def __init__(self, input_dim=5, hidden_dim=96, output_dim=192, num_layers=2):\n        super().__init__()\n        \n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n        \n        self.convs = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        \n        for _ in range(num_layers):\n            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n            self.norms.append(nn.LayerNorm(hidden_dim))\n        \n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, output_dim),\n            nn.LayerNorm(output_dim)\n        )\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        x = self.input_projection(x)\n        x = F.relu(x)\n        \n        for conv, norm in zip(self.convs, self.norms):\n            x_residual = x\n            x = conv(x, edge_index)\n            x = norm(x)\n            x = F.relu(x)\n            x = self.dropout(x)\n            x = x + x_residual\n        \n        x = global_mean_pool(x, batch)\n        x = self.output_projection(x)\n        \n        return x\n\n\nclass ProteinEncoder(nn.Module):\n    \"\"\"ESM-2 based protein encoder\"\"\"\n    \n    def __init__(self, model_name, output_dim=192, freeze=True):\n        super().__init__()\n        \n        self.esm = EsmModel.from_pretrained(model_name)\n        \n        if freeze:\n            for param in self.esm.parameters():\n                param.requires_grad = False\n        \n        esm_hidden_size = self.esm.config.hidden_size\n        \n        self.projection = nn.Sequential(\n            nn.Linear(esm_hidden_size, 384),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(384, output_dim),\n            nn.LayerNorm(output_dim)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.esm(input_ids=input_ids, attention_mask=attention_mask)\n        protein_embedding = outputs.last_hidden_state[:, 0, :]\n        protein_embedding = self.projection(protein_embedding)\n        \n        return protein_embedding, outputs.last_hidden_state\n\n\nclass BiDirectionalCrossAttention(nn.Module):\n    \"\"\"Bidirectional cross-attention\"\"\"\n    \n    def __init__(self, embed_dim=192, num_heads=6, dropout=0.15):\n        super().__init__()\n        \n        self.ligand_to_protein = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.protein_to_ligand = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n    \n    def forward(self, ligand_embed, protein_embed):\n        ligand_query = ligand_embed.unsqueeze(1)\n        protein_query = protein_embed.unsqueeze(1)\n        \n        lig_attn, lig_weights = self.ligand_to_protein(\n            query=ligand_query,\n            key=protein_query,\n            value=protein_query\n        )\n        lig_attn = self.norm1(lig_attn.squeeze(1) + ligand_embed)\n        \n        prot_attn, prot_weights = self.protein_to_ligand(\n            query=protein_query,\n            key=ligand_query,\n            value=ligand_query\n        )\n        prot_attn = self.norm2(prot_attn.squeeze(1) + protein_embed)\n        \n        return lig_attn, prot_attn, (lig_weights, prot_weights)\n\n\nclass DTAModel(nn.Module):\n    \"\"\"Complete Drug-Target Affinity prediction model\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.ligand_encoder = LigandGNN(\n            input_dim=5,\n            hidden_dim=config.GNN_HIDDEN_DIM,\n            output_dim=config.LIGAND_EMBED_DIM,\n            num_layers=config.GNN_NUM_LAYERS\n        )\n        \n        self.protein_encoder = ProteinEncoder(\n            model_name=config.ESM_MODEL,\n            output_dim=config.PROTEIN_EMBED_DIM,\n            freeze=config.FREEZE_ESM\n        )\n        \n        self.cross_attention = BiDirectionalCrossAttention(\n            embed_dim=config.LIGAND_EMBED_DIM,\n            num_heads=config.ATTENTION_HEADS,\n            dropout=config.ATTENTION_DROPOUT\n        )\n        \n        mlp_layers = []\n        for i in range(len(config.MLP_DIMS) - 1):\n            mlp_layers.extend([\n                nn.Linear(config.MLP_DIMS[i], config.MLP_DIMS[i+1]),\n                nn.ReLU() if i < len(config.MLP_DIMS) - 2 else nn.Identity(),\n                nn.Dropout(config.MLP_DROPOUT) if i < len(config.MLP_DIMS) - 2 else nn.Identity()\n            ])\n        \n        self.mlp = nn.Sequential(*mlp_layers)\n    \n    def forward(self, graph_batch, protein_input_ids, protein_attention_mask):\n        ligand_embed = self.ligand_encoder(graph_batch)\n        protein_embed, _ = self.protein_encoder(protein_input_ids, protein_attention_mask)\n        lig_attn, prot_attn, attn_weights = self.cross_attention(ligand_embed, protein_embed)\n        combined = torch.cat([lig_attn, prot_attn, protein_embed], dim=1)\n        pki_pred = self.mlp(combined).squeeze(-1)\n        \n        return pki_pred, attn_weights\n\n# ============================================================================\n# DATA PREPROCESSING (Same as original)\n# ============================================================================\n\ndef preprocess_and_cache_data(csv_path, config=Config):\n    \"\"\"\n    STEP 1: Run this to preprocess and cache data\n    Only needs to be run ONCE\n    \"\"\"\n    config.DATA_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n    cache_file = config.DATA_CACHE_DIR / config.PROCESSED_DATA_FILE\n    \n    if cache_file.exists():\n        print(\"=\" * 80)\n        print(\"CACHED DATA FOUND - Loading from disk...\")\n        print(\"=\" * 80)\n        with open(cache_file, 'rb') as f:\n            cached_data = pickle.load(f)\n        print(\"✓ Loaded cached data successfully!\")\n        return cached_data\n    \n    print(\"=\" * 80)\n    print(\"PREPROCESSING DATA - This will be saved to disk\")\n    print(\"=\" * 80)\n    \n    torch.manual_seed(config.RANDOM_SEED)\n    np.random.seed(config.RANDOM_SEED)\n    \n    print(\"Loading CSV...\")\n    df = pd.read_csv(csv_path)\n    print(f\"Total samples: {len(df)}\")\n    \n    original_len = len(df)\n    df = df[~df['pKi'].isna() & ~np.isinf(df['pKi'])]\n    if len(df) < original_len:\n        print(f\"⚠ Removed {original_len - len(df)} invalid pKi rows\")\n    \n    train_val_df, test_df = train_test_split(\n        df, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED\n    )\n    train_df, val_df = train_test_split(\n        train_val_df, test_size=config.VAL_SIZE / (1 - config.TEST_SIZE),\n        random_state=config.RANDOM_SEED\n    )\n    \n    print(f\"Split: Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\\n\")\n    \n    print(\"Loading ESM-2 tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(config.ESM_MODEL, trust_remote_code=True)\n    \n    print(\"\\nProcessing TRAIN dataset...\")\n    train_dataset = DTADataset(train_df, tokenizer, config.ESM_MAX_LENGTH)\n    \n    print(\"\\nProcessing VAL dataset...\")\n    val_dataset = DTADataset(\n        val_df, tokenizer, config.ESM_MAX_LENGTH,\n        pki_mean=train_dataset.pki_mean,\n        pki_std=train_dataset.pki_std\n    )\n    \n    print(\"\\nProcessing TEST dataset...\")\n    test_dataset = DTADataset(\n        test_df, tokenizer, config.ESM_MAX_LENGTH,\n        pki_mean=train_dataset.pki_mean,\n        pki_std=train_dataset.pki_std\n    )\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"SAVING PROCESSED DATA TO DISK...\")\n    cached_data = {\n        'train_cache': train_dataset.get_cache_data(),\n        'val_cache': val_dataset.get_cache_data(),\n        'test_cache': test_dataset.get_cache_data(),\n        'pki_mean': train_dataset.pki_mean,\n        'pki_std': train_dataset.pki_std,\n        'tokenizer_name': config.ESM_MODEL\n    }\n    \n    with open(cache_file, 'wb') as f:\n        pickle.dump(cached_data, f)\n    \n    print(f\"✓ Data saved to: {cache_file}\")\n    print(\"=\" * 80)\n    \n    return cached_data\n\n# ============================================================================\n# DDP TRAINER\n# ============================================================================\n\nclass DDPTrainer:\n    \"\"\"Distributed trainer using DistributedDataParallel\"\"\"\n    \n    def __init__(self, model, train_loader, val_loader, config, rank, \n                 pki_mean=0, pki_std=1):\n        self.config = config\n        self.rank = rank\n        self.pki_mean = pki_mean\n        self.pki_std = pki_std\n        \n        # Move model to GPU and wrap with DDP\n        self.model = model.to(rank)\n        self.model = DDP(\n            self.model, \n            device_ids=[rank],\n            find_unused_parameters=config.FIND_UNUSED_PARAMETERS\n        )\n        \n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        \n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=config.LEARNING_RATE,\n            weight_decay=config.WEIGHT_DECAY\n        )\n        \n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            mode='min',\n            factor=0.5,\n            patience=5,\n            verbose=(rank == 0)\n        )\n        \n        self.scaler = torch.cuda.amp.GradScaler()\n        self.criterion = nn.MSELoss()\n        self.best_val_loss = float('inf')\n        self.patience_counter = 0\n    \n    def train_epoch(self):\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n        self.optimizer.zero_grad()\n        \n        pbar = tqdm(self.train_loader, desc='Training', disable=(self.rank != 0))\n        \n        for batch_idx, batch in enumerate(pbar):\n            graph_batch = batch['graph_batch'].to(self.rank)\n            protein_input_ids = batch['protein_input_ids'].to(self.rank)\n            protein_attention_mask = batch['protein_attention_mask'].to(self.rank)\n            pki_true = batch['pki'].to(self.rank)\n            \n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast():\n                pki_pred, _ = self.model(graph_batch, protein_input_ids, protein_attention_mask)\n                loss = self.criterion(pki_pred, pki_true)\n                loss = loss / self.config.GRADIENT_ACCUMULATION\n            \n            # Backward pass\n            self.scaler.scale(loss).backward()\n            \n            # Gradient accumulation\n            if (batch_idx + 1) % self.config.GRADIENT_ACCUMULATION == 0 or \\\n               (batch_idx + 1) == len(self.train_loader):\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(\n                    self.model.parameters(), \n                    self.config.GRAD_CLIP_NORM\n                )\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.optimizer.zero_grad()\n            \n            total_loss += loss.item() * self.config.GRADIENT_ACCUMULATION\n            num_batches += 1\n            \n            if self.rank == 0:\n                pbar.set_postfix({'loss': f'{loss.item() * self.config.GRADIENT_ACCUMULATION:.4f}'})\n        \n        # Average loss across all GPUs\n        avg_loss = total_loss / num_batches\n        avg_loss = reduce_metric(avg_loss, get_world_size())\n        \n        return avg_loss\n    \n    def evaluate(self, loader):\n        self.model.eval()\n        total_loss = 0\n        all_preds = []\n        all_trues = []\n        \n        with torch.no_grad():\n            pbar = tqdm(loader, desc='Evaluating', disable=(self.rank != 0))\n            for batch in pbar:\n                graph_batch = batch['graph_batch'].to(self.rank)\n                protein_input_ids = batch['protein_input_ids'].to(self.rank)\n                protein_attention_mask = batch['protein_attention_mask'].to(self.rank)\n                pki_true = batch['pki'].to(self.rank)\n                \n                with torch.cuda.amp.autocast():\n                    pki_pred, _ = self.model(graph_batch, protein_input_ids, protein_attention_mask)\n                \n                if torch.isnan(pki_pred).any() or torch.isinf(pki_pred).any():\n                    pki_pred = torch.nan_to_num(pki_pred, nan=0.0, posinf=10.0, neginf=0.0)\n                \n                loss = self.criterion(pki_pred, pki_true)\n                \n                total_loss += loss.item()\n                all_preds.extend(pki_pred.cpu().numpy())\n                all_trues.extend(pki_true.cpu().numpy())\n        \n        # Gather predictions from all GPUs\n        all_preds, all_trues = all_gather_predictions(all_preds, all_trues)\n        \n        # Denormalize\n        all_preds = np.array(all_preds) * self.pki_std + self.pki_mean\n        all_trues = np.array(all_trues) * self.pki_std + self.pki_mean\n        \n        all_preds = np.nan_to_num(all_preds, nan=0.0, posinf=100.0, neginf=0.0)\n        all_trues = np.nan_to_num(all_trues, nan=0.0, posinf=100.0, neginf=0.0)\n        \n        avg_loss = reduce_metric(total_loss / len(loader), get_world_size())\n        \n        if self.rank == 0:\n            r2 = r2_score(all_trues, all_preds)\n            rmse = np.sqrt(mean_squared_error(all_trues, all_preds))\n            mae = mean_absolute_error(all_trues, all_preds)\n        else:\n            r2 = rmse = mae = 0.0\n        \n        return avg_loss, r2, rmse, mae\n    \n    def train(self):\n        if self.rank == 0:\n            print(f\"\\nDistributed Training on {get_world_size()} GPUs\")\n            print(f\"Per-GPU batch size: {self.config.BATCH_SIZE}\")\n            print(f\"Effective batch size: {self.config.BATCH_SIZE * get_world_size() * self.config.GRADIENT_ACCUMULATION}\")\n            print(\"=\" * 80)\n        \n        for epoch in range(self.config.NUM_EPOCHS):\n            # Set epoch for DistributedSampler\n            self.train_loader.sampler.set_epoch(epoch)\n            \n            if self.rank == 0:\n                print(f\"\\nEpoch {epoch + 1}/{self.config.NUM_EPOCHS}\")\n            \n            train_loss = self.train_epoch()\n            val_loss, val_r2, val_rmse, val_mae = self.evaluate(self.val_loader)\n            \n            if self.rank == 0:\n                self.scheduler.step(val_loss)\n                \n                print(f\"Train Loss: {train_loss:.4f}\")\n                print(f\"Val Loss: {val_loss:.4f} | R²: {val_r2:.4f} | RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f}\")\n                \n                if val_loss < self.best_val_loss:\n                    self.best_val_loss = val_loss\n                    self.patience_counter = 0\n                    \n                    # Save model (only rank 0)\n                    checkpoint = {\n                        'epoch': epoch + 1,\n                        'model_state_dict': self.model.module.state_dict(),  # Use .module for DDP\n                        'optimizer_state_dict': self.optimizer.state_dict(),\n                        'val_loss': val_loss,\n                        'val_r2': val_r2,\n                        'pki_mean': self.pki_mean,\n                        'pki_std': self.pki_std\n                    }\n                    torch.save(checkpoint, 'best_model_checkpoint.pt')\n                    torch.save(self.model.module.state_dict(), 'best_model_weights.pt')\n                    print(\"✓ Model saved!\")\n                else:\n                    self.patience_counter += 1\n                    if self.patience_counter >= self.config.PATIENCE:\n                        print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n                        break\n            \n            # Synchronize early stopping across all processes\n            if dist.is_initialized():\n                should_stop = torch.tensor([self.patience_counter >= self.config.PATIENCE], \n                                          device=self.rank, dtype=torch.bool)\n                dist.broadcast(should_stop, src=0)\n                if should_stop.item():\n                    break\n            \n            # Memory cleanup\n            torch.cuda.empty_cache()\n        \n        # Wait for all processes\n        if dist.is_initialized():\n            dist.barrier()\n        \n        # Load best model on rank 0\n        if self.rank == 0:\n            self.model.module.load_state_dict(torch.load('best_model_weights.pt'))\n            print(\"\\n\" + \"=\" * 80)\n            print(\"Training completed!\")\n\n\n# ============================================================================\n# MAIN DDP TRAINING FUNCTION\n# ============================================================================\n\ndef train_worker(rank, world_size, cached_data, config):\n    \"\"\"\n    Worker function for each GPU process\n    This is called by torch.multiprocessing.spawn\n    \"\"\"\n    # Setup DDP\n    setup_ddp(rank, world_size)\n    \n    if rank == 0:\n        print(\"=\" * 80)\n        print(f\"GPU INFORMATION\")\n        print(\"=\" * 80)\n        for i in range(world_size):\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n            print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n        print(\"=\" * 80 + \"\\n\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        cached_data['tokenizer_name'], \n        trust_remote_code=True\n    )\n    \n    # Create datasets from cache\n    train_dataset = DTADataset(\n        pd.DataFrame(), tokenizer, config.ESM_MAX_LENGTH,\n        normalize_pki=True,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std'],\n        cache_data=cached_data['train_cache']\n    )\n    \n    val_dataset = DTADataset(\n        pd.DataFrame(), tokenizer, config.ESM_MAX_LENGTH,\n        normalize_pki=True,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std'],\n        cache_data=cached_data['val_cache']\n    )\n    \n    test_dataset = DTADataset(\n        pd.DataFrame(), tokenizer, config.ESM_MAX_LENGTH,\n        normalize_pki=True,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std'],\n        cache_data=cached_data['test_cache']\n    )\n    \n    if rank == 0:\n        print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\\n\")\n    \n    # Create DistributedSamplers\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True,\n        seed=config.RANDOM_SEED\n    )\n    \n    val_sampler = DistributedSampler(\n        val_dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=False\n    )\n    \n    test_sampler = DistributedSampler(\n        test_dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=False\n    )\n    \n    # Create dataloaders with DistributedSampler\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.BATCH_SIZE,\n        sampler=train_sampler,  # Use sampler instead of shuffle\n        collate_fn=collate_fn,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=config.PIN_MEMORY,\n        persistent_workers=config.PERSISTENT_WORKERS\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.BATCH_SIZE,\n        sampler=val_sampler,\n        collate_fn=collate_fn,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=config.PIN_MEMORY,\n        persistent_workers=config.PERSISTENT_WORKERS\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config.BATCH_SIZE,\n        sampler=test_sampler,\n        collate_fn=collate_fn,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=config.PIN_MEMORY,\n        persistent_workers=config.PERSISTENT_WORKERS\n    )\n    \n    # Initialize model\n    if rank == 0:\n        print(\"Initializing model...\")\n    \n    model = DTAModel(config)\n    \n    # Better initialization\n    def init_weights(m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight, gain=0.5)\n            if m.bias is not None:\n                torch.nn.init.zeros_(m.bias)\n    \n    model.apply(init_weights)\n    \n    if rank == 0:\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\\n\")\n    \n    # Train\n    trainer = DDPTrainer(\n        model, train_loader, val_loader, config, rank,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std']\n    )\n    trainer.train()\n    \n    # Final test evaluation (only on rank 0)\n    if rank == 0:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"FINAL TEST SET EVALUATION\")\n        print(\"=\" * 80)\n    \n    test_loss, test_r2, test_rmse, test_mae = trainer.evaluate(test_loader)\n    \n    if rank == 0:\n        print(f\"Test Loss: {test_loss:.4f}\")\n        print(f\"Test R²: {test_r2:.4f}\")\n        print(f\"Test RMSE: {test_rmse:.4f}\")\n        print(f\"Test MAE: {test_mae:.4f}\")\n        print(\"=\" * 80)\n    \n    # Cleanup\n    cleanup_ddp()\n\n\ndef train_from_cache_ddp(cache_path=None, config=Config):\n    \"\"\"\n    STEP 2: Run this to train using DDP with cached data\n    \"\"\"\n    if cache_path is None:\n        # Make sure this path is correct for your Kaggle environment\n        # For example, if your cached data is in /kaggle/input/your-dataset/\n        cache_path = Path('/kaggle/input/cache-data/processed_datasets.pkl')\n    else:\n        cache_path = Path(cache_path)\n    \n    if not cache_path.exists():\n        raise FileNotFoundError(\n            f\"No cached data found at {cache_path}! Run preprocess_and_cache_data() first.\"\n        )\n    \n    print(\"=\" * 80)\n    print(\"LOADING CACHED DATA FOR DDP TRAINING\")\n    print(\"=\" * 80)\n    \n    with open(cache_path, 'rb') as f:\n        cached_data = pickle.load(f)\n    \n    print(\"✓ Cached data loaded!\")\n    \n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is not available. DDP requires GPUs.\")\n    \n    world_size = torch.cuda.device_count()\n    \n    if world_size < 2:\n        print(\"\\n⚠ WARNING: Only 1 GPU detected. DDP is not needed.\")\n        # ... (rest of the warning)\n    \n    print(f\"\\nDetected {world_size} GPUs. Starting distributed training...\")\n    print(\"=\" * 80 + \"\\n\")\n\n    # ========================================================================\n    # CRITICAL FIX: SET THE START METHOD HERE\n    # ========================================================================\n    # This MUST be placed right before mp.spawn in the main script execution.\n    try:\n        mp.set_start_method('fork', force=True)\n        print(\"✓ Set multiprocessing start method to 'fork'.\")\n    except RuntimeError:\n        # This might happen in some interactive environments if the context is already set.\n        # It's usually safe to ignore.\n        print(\"Multiprocessing context already set. Continuing...\")\n    # ========================================================================\n\n    # Spawn processes for each GPU\n    mp.spawn(\n        train_worker,\n        args=(world_size, cached_data, config),\n        nprocs=world_size,\n        join=True\n    )\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"DDP TRAINING COMPLETED!\")\n    print(\"=\" * 80)\n\n\n\n# ============================================================================\n# CONVENIENCE FUNCTIONS\n# ============================================================================\n\ndef train_single_gpu_from_cache(cache_path=None, config=Config):\n    \"\"\"\n    Alternative: Single GPU training (more stable for debugging)\n    \n    Use this if you encounter DDP issues or want to debug\n    \"\"\"\n    if cache_path is None:\n        cache_path = Path('/kaggle/input/cache-data/processed_datasets.pkl')\n    else:\n        cache_path = Path(cache_path)\n    \n    if not cache_path.exists():\n        raise FileNotFoundError(\n            f\"No cached data found at {cache_path}! Run preprocess_and_cache_data() first.\"\n        )\n    \n    print(\"=\" * 80)\n    print(\"LOADING CACHED DATA FOR SINGLE-GPU TRAINING\")\n    print(\"=\" * 80)\n    \n    with open(cache_path, 'rb') as f:\n        cached_data = pickle.load(f)\n    \n    print(\"✓ Cached data loaded!\")\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(f\"\\nUsing device: {device}\")\n    \n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        cached_data['tokenizer_name'], \n        trust_remote_code=True\n    )\n    \n    # Create datasets\n    train_dataset = DTADataset(\n        pd.DataFrame(), tokenizer, config.ESM_MAX_LENGTH,\n        normalize_pki=True,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std'],\n        cache_data=cached_data['train_cache']\n    )\n    \n    val_dataset = DTADataset(\n        pd.DataFrame(), tokenizer, config.ESM_MAX_LENGTH,\n        normalize_pki=True,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std'],\n        cache_data=cached_data['val_cache']\n    )\n    \n    test_dataset = DTADataset(\n        pd.DataFrame(), tokenizer, config.ESM_MAX_LENGTH,\n        normalize_pki=True,\n        pki_mean=cached_data['pki_mean'],\n        pki_std=cached_data['pki_std'],\n        cache_data=cached_data['test_cache']\n    )\n    \n    print(f\"\\nTrain: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\\n\")\n    \n    # Create dataloaders (no DistributedSampler)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.BATCH_SIZE,\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=config.PIN_MEMORY\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=config.PIN_MEMORY\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=config.NUM_WORKERS,\n        pin_memory=config.PIN_MEMORY\n    )\n    \n    # Initialize model\n    print(\"Initializing model...\")\n    model = DTAModel(config).to(device)\n    \n    def init_weights(m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight, gain=0.5)\n            if m.bias is not None:\n                torch.nn.init.zeros_(m.bias)\n    \n    model.apply(init_weights)\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\\n\")\n    \n    # Use a simplified trainer for single GPU\n    from types import SimpleNamespace\n    single_config = SimpleNamespace(**vars(config))\n    single_config.DEVICE = device\n    \n    # Note: You would need to implement a SingleGPUTrainer or adapt the original Trainer class\n    print(\"For single-GPU training, use the original Trainer class from the source code.\")\n    print(\"This function serves as a template.\")\n\n\n# ============================================================================\n# USAGE EXAMPLES\n# ============================================================================\n\n\"\"\"\nUSAGE:\n\n# Step 1: Preprocess data (run once)\ncached_data = preprocess_and_cache_data('/path/to/your/data.csv')\n\n# Step 2: Train with DDP (multiple GPUs)\ntrain_from_cache_ddp()\n\n# Alternative: Train with single GPU (for debugging)\ntrain_single_gpu_from_cache()\n\n# To resume training or change config:\nconfig = Config()\nconfig.LEARNING_RATE = 1e-5\nconfig.NUM_EPOCHS = 100\ntrain_from_cache_ddp(config=config)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:46:02.665837Z","iopub.execute_input":"2025-10-13T14:46:02.666511Z","iopub.status.idle":"2025-10-13T14:46:02.774745Z","shell.execute_reply.started":"2025-10-13T14:46:02.666488Z","shell.execute_reply":"2025-10-13T14:46:02.774125Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"\"\\nUSAGE:\\n\\n# Step 1: Preprocess data (run once)\\ncached_data = preprocess_and_cache_data('/path/to/your/data.csv')\\n\\n# Step 2: Train with DDP (multiple GPUs)\\ntrain_from_cache_ddp()\\n\\n# Alternative: Train with single GPU (for debugging)\\ntrain_single_gpu_from_cache()\\n\\n# To resume training or change config:\\nconfig = Config()\\nconfig.LEARNING_RATE = 1e-5\\nconfig.NUM_EPOCHS = 100\\ntrain_from_cache_ddp(config=config)\\n\""},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_from_cache_ddp()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:46:10.463516Z","iopub.execute_input":"2025-10-13T14:46:10.463831Z","iopub.status.idle":"2025-10-13T14:46:19.559145Z","shell.execute_reply.started":"2025-10-13T14:46:10.463807Z","shell.execute_reply":"2025-10-13T14:46:19.554459Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nLOADING CACHED DATA FOR DDP TRAINING\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/673258682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_from_cache_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/2981935273.py\u001b[0m in \u001b[0;36mtrain_from_cache_ddp\u001b[0;34m(cache_path, config)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0mcached_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ Cached data loaded!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMemoryError\u001b[0m: "],"ename":"MemoryError","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import gc\n\n# Run garbage collector\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:43:47.594685Z","iopub.status.idle":"2025-10-13T14:43:47.595004Z","shell.execute_reply.started":"2025-10-13T14:43:47.594856Z","shell.execute_reply":"2025-10-13T14:43:47.594874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This loads from cache and trains\n# No reprocessing needed, even after crashes!\nmodel, trainer = train_from_cache()\n\n# Model checkpoints saved:\n# - best_model_weights.pt\n# - best_model_checkpoint.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:43:47.596536Z","iopub.status.idle":"2025-10-13T14:43:47.596993Z","shell.execute_reply.started":"2025-10-13T14:43:47.596837Z","shell.execute_reply":"2025-10-13T14:43:47.596855Z"}},"outputs":[],"execution_count":null}]}